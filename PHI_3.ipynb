{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911fa5b3b0ea490aae6a481be84b66c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 14.01s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-128k-instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "#    device_map=\"cuda\",\n",
    "    attn_implementation=\"eager\",\n",
    ")\n",
    "\n",
    "#'''\n",
    "#If you want to run the model on:\n",
    "#NVIDIA V100 or earlier generation GPUs: call AutoModelForCausalLM.from_pretrained() with attn_implementation=\"eager\"\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32010, 29871,    13, 29911,   514,   592,  1048, 22746,   398,\n",
       "          6189, 29257, 29973, 32007, 29871,    13, 32001, 29871,    13]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "     \n",
    "\n",
    "chat = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tell me about Quantum Machine Learning?\",\n",
    "    }\n",
    "]\n",
    "     \n",
    "\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize = False, add_generation_prompt = True)\n",
    "     \n",
    "\n",
    "token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "     \n",
    "\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  output_ids = model.generate(\n",
    "      token_ids.to(model.device),\n",
    "      do_sample = True,\n",
    "      temperature = 0.7,\n",
    "      max_new_tokens = 512\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum Machine Learning (QML) is an emerging field that combines quantum physics and machine learning. It aims to harness the power of quantum computing to improve upon traditional machine learning algorithms. \n",
      "\n",
      "1. Quantum Computing Basics: Quantum computers operate differently from classical computers. They use quantum bits (qubits) instead of bits. These qubits can exist in multiple states at once, a phenomenon known as superposition. Furthermore, qubits can be entangled, meaning the state of one qubit can instantly affect the state of another, no matter the distance between them.\n",
      "\n",
      "2. Machine Learning Basics: Machine learning, a subset of artificial intelligence (AI), involves algorithms that improve automatically through experience and adapt to new inputs. Machine learning algorithms can recognize patterns and make predictions based on data.\n",
      "\n",
      "3. Quantum Machine Learning: In QML, quantum algorithms are developed to improve the efficiency of machine learning tasks. These algorithms leverage the properties of quantum systems to process and analyze data at an unprecedented speed. They aim to solve complex problems more efficiently than classical machine learning algorithms, particularly those that require substantial computational power.\n",
      "\n",
      "4. Applications: Despite being in its early stages, QML has potential applications in various fields such as drug discovery, optimization problems, financial modeling, climate modeling, and more. For instance, it could be used to analyze molecular structures for drug discovery which could lead to breakthroughs in personalized medicine.\n",
      "\n",
      "5. Challenges: There are several challenges to overcome in QML. Quantum hardware is still in its nascent stage, meaning there are issues with stability, noise, and error rates. Moreover, quantum algorithms are complex and require specialized knowledge to develop.\n",
      "\n",
      "In conclusion, Quantum Machine Learning is a promising new field that merges the principles of quantum physics with machine learning to potentially revolutionize data processing and problem-solving.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
