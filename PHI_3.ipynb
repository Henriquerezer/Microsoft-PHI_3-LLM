{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911fa5b3b0ea490aae6a481be84b66c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 14.01s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-128k-instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "#    device_map=\"cuda\",\n",
    "    attn_implementation=\"eager\",\n",
    ")\n",
    "\n",
    "#'''\n",
    "#If you want to run the model on:\n",
    "#NVIDIA V100 or earlier generation GPUs: call AutoModelForCausalLM.from_pretrained() with attn_implementation=\"eager\"\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32010, 29871,    13,  6816, 21030,  4166,   288,  3382,  7078,\n",
       "           316,   365, 26369,  1146,  7783,  1963, 29875, 29899, 29941, 29899,\n",
       "          1195, 29875, 29899, 29896, 29906, 29947, 29895, 29899,  2611,  1247,\n",
       "         29973, 32007, 29871,    13, 32001, 29871,    13]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "     \n",
    "\n",
    "chat = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Me conte sobre o Modelo de LLM da Microsoft Phi-3-mini-128k-instruct?\",\n",
    "    }\n",
    "]\n",
    "     \n",
    "\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize = False, add_generation_prompt = True)\n",
    "     \n",
    "\n",
    "token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "     \n",
    "\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  output_ids = model.generate(\n",
    "      token_ids.to(model.device),\n",
    "      do_sample = True,\n",
    "      temperature = 0.7,\n",
    "      max_new_tokens = 512\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Modelo de Linguagem de Inteligência Artificial (LLM) da Microsoft Phi-3-mini-128k é uma versão reduzida do Phi-3 de Microsoft. Embora seja menor em termos de tamanho - 128k em comparação com o Phi-3 -, ele ainda é capaz de entender e processar linguagem humana. Ele pode completar tarefas relacionadas à linguagem como tradução, resposta a perguntas e assistente de escrita. No entanto, como é uma versão menor, pode não ter a capacidade de capturar tanta complexidade de linguagem humana ou realizar tarefas mais avançadas que um modelo maior.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
